{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c08e859-2b87-429e-aa35-125c007cb664",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üßº Nettoyage et pr√©paration du dataset Spotify\n",
    "\n",
    "Dans ce bloc de code, nous pr√©parons les donn√©es afin de les rendre exploitables pour l‚Äôanalyse et la mod√©lisation.\n",
    "\n",
    "1. **S√©lection des colonnes pertinentes**\n",
    "   On conserve uniquement les colonnes utiles pour la pr√©diction de la popularit√© d‚Äôune chanson. Cela inclut des caract√©ristiques audio (`Tempo`, `Loudness`, `Danceability`, etc.), des m√©tadonn√©es (`Genre`, `emotion`) ainsi que la variable cible `Popularity`.\n",
    "\n",
    "2. **Conversion de la dur√©e en secondes**\n",
    "   La colonne `\"Length\"` est au format `\"minutes:secondes\"` (ex: `\"3:45\"`). On la convertit en une nouvelle colonne `\"Length_sec\"` exprim√©e en secondes. L‚Äôancienne colonne `\"Length\"` est ensuite supprim√©e.\n",
    "\n",
    "3. **Normalisation des colonnes en pourcentage**\n",
    "   Certaines colonnes repr√©sentent des pourcentages compris entre 0 et 100 (par exemple : `Energy`, `Danceability`). On les divise par 100 pour ramener leurs valeurs dans l‚Äôintervalle `[0, 1]`, ce qui facilite l'apprentissage des mod√®les.\n",
    "\n",
    "4. **Suppression des valeurs manquantes**\n",
    "   Toutes les lignes contenant des donn√©es manquantes (`NaN`) sont supprim√©es afin d‚Äô√©viter des erreurs lors de l'entra√Ænement des mod√®les.\n",
    "\n",
    "5. **Aper√ßu des donn√©es**\n",
    "   On affiche les 20 premi√®res lignes du dataset nettoy√© pour v√©rifier que tout est correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5a31490a-4cdf-4e48-a560-0a0e1fcea218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Dataset/spotify_dataset_popularity_balanced.csv\")  # Remplace par le bon chemin si besoin\n",
    "# # Chargement du dataset\n",
    "# df = pd.read_csv(\"Dataset/spotify_dataset.csv\")  # Remplace par le bon chemin si besoin\n",
    "\n",
    "\n",
    "# def length_to_seconds(length):\n",
    "#     if pd.notna(length):\n",
    "#         parts = str(length).split(\":\")\n",
    "#         if len(parts) == 2:\n",
    "#             minutes = int(parts[0])\n",
    "#             seconds = int(parts[1])\n",
    "#             return minutes * 60 + seconds\n",
    "#     return 0\n",
    "\n",
    "# df['text'] = df['text'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0) #A R√©√©crire\n",
    "# df['Length'] = df['Length'].apply(length_to_seconds) #Conversion du Length en secondes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7a623160-5385-4323-bcfe-c9e627512eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # S√©parer les morceaux populaires et non populaires\n",
    "# popular = df[df[\"Popularity\"] > 70]\n",
    "# non_popular = df[df[\"Popularity\"] <= 70]\n",
    "\n",
    "# # Objectif : 40% de morceaux populaires, 60% de non populaires\n",
    "# nb_popular = 8000\n",
    "# nb_non_popular = int((0.6 * nb_popular) / 0.4)  # soit 15000\n",
    "\n",
    "# # √âchantillonnage al√©atoire\n",
    "# popular_sampled = popular.sample(n=nb_popular, random_state=42)\n",
    "# non_popular_sampled = non_popular.sample(n=nb_non_popular, random_state=42)\n",
    "\n",
    "# # Fusionner et m√©langer\n",
    "# balanced_df = pd.concat([popular_sampled, non_popular_sampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # V√©rification\n",
    "# l1 = len(balanced_df[balanced_df[\"Popularity\"] > 70])\n",
    "# l2 = len(balanced_df[balanced_df[\"Popularity\"] <= 70])\n",
    "# print(\"Total :\", len(balanced_df))\n",
    "# print(\"Popularit√© > 70 :\", l1)\n",
    "# print(\"Popularit√© ‚â§ 70 :\", l2)\n",
    "# print(\"Proportions :\", round(l1 / (l1 + l2), 2), \"/\", round(l2 / (l1 + l2), 2))\n",
    "\n",
    "# df.to_csv(\"Dataset/spotify_dataset_popularity_balanced.csv\", index=False)\n",
    "\n",
    "# df = balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d83d3-4550-4ff4-aa79-9c9c2f8366f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "3987676d-af9f-45eb-a374-0475f371d867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeu de donn√©es nettoy√© :\n",
      "     emotion    Genre     Tempo  Loudness (db)  Energy  Danceability  \\\n",
      "0    sadness  hip hop  0.437870       0.785065    0.83          0.71   \n",
      "1    sadness  hip hop  0.508876       0.805051    0.85          0.70   \n",
      "2        joy  hip hop  0.532544       0.799419    0.89          0.71   \n",
      "3        joy  hip hop  0.538462       0.811047    0.84          0.78   \n",
      "4        joy  hip hop  0.544379       0.808321    0.71          0.77   \n",
      "5       love  hip hop  0.538462       0.782340    0.81          0.87   \n",
      "6    sadness  hip hop  0.431953       0.784884    0.89          0.68   \n",
      "7        joy  hip hop  0.526627       0.818677    0.88          0.77   \n",
      "8   surprise  hip hop  0.526627       0.750908    0.72          0.86   \n",
      "9    sadness  hip hop  0.550296       0.759811    0.68          0.77   \n",
      "10   sadness  hip hop  0.390533       0.766897    0.76          0.76   \n",
      "11  surprise  hip hop  0.520710       0.790153    0.93          0.63   \n",
      "12       joy  hip hop  0.562130       0.800327    0.80          0.69   \n",
      "13       joy  hip hop  0.550296       0.774164    0.83          0.79   \n",
      "14     anger  hip hop  0.538462       0.802507    0.99          0.50   \n",
      "15     anger  hip hop  0.538462       0.777071    0.92          0.74   \n",
      "16      fear  hip hop  0.526627       0.800509    0.76          0.86   \n",
      "17   sadness  hip hop  0.603550       0.786337    0.89          0.78   \n",
      "18     anger  hip hop  0.852071       0.764353    0.40          0.72   \n",
      "19     anger  hip hop  0.325444       0.719840    0.55          0.52   \n",
      "\n",
      "    Positiveness  Speechiness  Liveness  Acousticness  Instrumentalness  \\\n",
      "0           0.87         0.04      0.16          0.11              0.00   \n",
      "1           0.87         0.04      0.32          0.00              0.00   \n",
      "2           0.63         0.08      0.64          0.00              0.20   \n",
      "3           0.97         0.04      0.12          0.12              0.00   \n",
      "4           0.70         0.07      0.10          0.04              0.01   \n",
      "5           0.74         0.04      0.07          0.00              0.47   \n",
      "6           0.65         0.08      0.09          0.00              0.00   \n",
      "7           0.95         0.05      0.34          0.00              0.04   \n",
      "8           0.58         0.10      0.22          0.06              0.24   \n",
      "9           0.67         0.04      0.10          0.05              0.00   \n",
      "10          0.88         0.03      0.27          0.13              0.68   \n",
      "11          0.40         0.05      0.12          0.00              0.10   \n",
      "12          0.46         0.07      0.06          0.03              0.03   \n",
      "13          0.63         0.09      0.11          0.33              0.04   \n",
      "14          0.61         0.08      0.08          0.00              0.05   \n",
      "15          0.77         0.13      0.11          0.01              0.01   \n",
      "16          0.80         0.05      0.06          0.00              0.32   \n",
      "17          0.80         0.07      0.13          0.05              0.00   \n",
      "18          0.52         0.28      0.12          0.00              0.00   \n",
      "19          0.78         0.15      0.27          0.45              0.00   \n",
      "\n",
      "    Length  Popularity  \n",
      "0      227          40  \n",
      "1      243          42  \n",
      "2      351          29  \n",
      "3      224          24  \n",
      "4      360          30  \n",
      "5      322          26  \n",
      "6      219          17  \n",
      "7      357          27  \n",
      "8      254          33  \n",
      "9      453          21  \n",
      "10     143          34  \n",
      "11     362          23  \n",
      "12     313          26  \n",
      "13     261          20  \n",
      "14     184          23  \n",
      "15     215          18  \n",
      "16     281          36  \n",
      "17     270          31  \n",
      "18     122           0  \n",
      "19      75           4  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "colonnes_utiles = [\n",
    "    \"emotion\", \"Genre\", \"Tempo\", \"Loudness (db)\", \"Energy\", \"Danceability\",\n",
    "    \"Positiveness\", \"Speechiness\", \"Liveness\", \"Acousticness\", \"Instrumentalness\",\n",
    "    \"Length\", \"Popularity\"\n",
    "]\n",
    "df = df[colonnes_utiles]\n",
    "\n",
    "\n",
    "\n",
    "#df.drop(columns=[\"Length\"], inplace=True)\n",
    "\n",
    "pourcentages = [\"Energy\", \"Danceability\", \"Positiveness\", \"Speechiness\", \"Liveness\", \"Acousticness\", \"Instrumentalness\"]\n",
    "for col in pourcentages:\n",
    "    df[col] = df[col] / 100.0\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"Jeu de donn√©es nettoy√© :\")\n",
    "print(df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4cfee-9a39-4694-a42a-d3fc6dcecf6c",
   "metadata": {},
   "source": [
    "### üîç Entra√Ænement et √©valuation de plusieurs mod√®les de r√©gression\n",
    "\n",
    "Dans cette section, nous pr√©parons les donn√©es et √©valuons plusieurs mod√®les de r√©gression afin de pr√©dire la popularit√© d'une chanson.\n",
    "\n",
    "1. **Pr√©paration des donn√©es**\n",
    "\n",
    "   * La variable cat√©gorielle `\"emotion\"` est encod√©e avec un **OneHotEncoder** afin de la transformer en variables num√©riques.\n",
    "   * La variable cible est `Popularity`, que l‚Äôon cherche √† pr√©dire.\n",
    "   * Les donn√©es sont divis√©es en un **ensemble d‚Äôentra√Ænement (80%)** et un **ensemble de test (20%)**.\n",
    "\n",
    "2. **Mod√®les test√©s**\n",
    "\n",
    "   * **R√©gression lin√©aire** simple.\n",
    "   * **R√©gression Ridge** (lin√©aire r√©gularis√©e).\n",
    "   * **R√©gression polynomiale** (de degr√© 2 √† 3), qui permet de mod√©liser des relations non lin√©aires entre les variables.\n",
    "   * **Random Forest Regressor**, un mod√®le bas√© sur des arbres de d√©cision.\n",
    "   * **R√©gression logistique**, utilis√©e ici non pas pour la r√©gression mais pour la **classification binaire** : une chanson est consid√©r√©e comme *populaire* si sa popularit√© est sup√©rieure ou √©gale √† 50.\n",
    "\n",
    "3. **√âvaluation**\n",
    "\n",
    "   * Pour les mod√®les de **r√©gression**, on √©value les performances avec :\n",
    "\n",
    "     * **RMSE** (Root Mean Squared Error) : plus elle est faible, mieux c‚Äôest.\n",
    "     * **R¬≤** (coefficient de d√©termination) : mesure la qualit√© de l‚Äôajustement (proche de 1 = bon mod√®le).\n",
    "   * Pour la **r√©gression logistique**, on √©value l‚Äô**accuracy** (taux de bonnes classifications).\n",
    "\n",
    "Ce processus permet de comparer diff√©rentes approches et d‚Äôidentifier les mod√®les les plus adapt√©s pour pr√©dire ou classifier la popularit√© musicale √† partir des caract√©ristiques audio et √©motionnelles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00727a5-2001-463e-ae60-ca388a441ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ R√©gression Lin√©aire\n",
      "RMSE : 16.89\n",
      "R¬≤ : 0.032\n",
      "\n",
      "üîπ R√©gression Ridge\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# 1. S√©parer les variables\n",
    "emotion = df[[\"emotion\"]]\n",
    "X_rest = df.drop(columns=[\"emotion\", \"Genre\", \"Popularity\"])\n",
    "y = df[\"Popularity\"]\n",
    "\n",
    "# 2. Encoder \"emotion\"\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "emotion_encoded = encoder.fit_transform(emotion)\n",
    "emotion_df = pd.DataFrame(\n",
    "    emotion_encoded,\n",
    "    columns=encoder.get_feature_names_out([\"emotion\"]),\n",
    "    index=emotion.index\n",
    ")\n",
    "\n",
    "# 3. Concat√©ner les features num√©riques et encod√©es\n",
    "X = pd.concat([X_rest, emotion_df], axis=1)\n",
    "\n",
    "# 4. S√©parer les donn√©es en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fonction de pond√©ration gaussienne\n",
    "def gaussian_weights(distances):\n",
    "    sigma = np.std(distances)\n",
    "    return np.exp(- (distances ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "# 5. D√©finir les mod√®les\n",
    "models = {\n",
    "    \"R√©gression Lin√©aire\": LinearRegression(),\n",
    "    \"R√©gression Ridge\": Ridge(alpha=1.0, solver='sag'),  # ‚úÖ non modifi√©\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"R√©gression Logistique\": LogisticRegression(max_iter=1000),\n",
    "    \"R√©gression KNN\": KNeighborsRegressor(n_neighbors=5),\n",
    "}\n",
    "\n",
    "# Ajouter automatiquement des mod√®les polynomiaux (degr√©s 2 √† 5)\n",
    "for deg in range(2, 4):\n",
    "    models[f\"R√©gression Polynomiale (deg={deg})\"] = make_pipeline(\n",
    "        PolynomialFeatures(degree=deg),\n",
    "        LinearRegression()\n",
    "    )\n",
    "\n",
    "# 6. Binariser la target pour la classification\n",
    "y_class = (y >= 50).astype(int)\n",
    "\n",
    "# 7. Entra√Æner et √©valuer\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîπ {name}\")\n",
    "    if \"Logistique\" in name:\n",
    "        model.fit(X_train, y_class.loc[X_train.index])\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        acc = accuracy_score(y_class.loc[X_test.index], y_pred)\n",
    "        print(\"Accuracy (classification binaire) :\", round(acc, 3))\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(\"RMSE :\", round(rmse, 2))\n",
    "        print(\"R¬≤ :\", round(r2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9614da3a-0451-4ad4-8cd9-972c120a9cf1",
   "metadata": {},
   "source": [
    "\n",
    "### üìä Analyse des erreurs des mod√®les de r√©gression\n",
    "\n",
    "Ce bloc de code permet de visualiser les **erreurs absolues** commises par chaque mod√®le de r√©gression sur les pr√©dictions de popularit√©.\n",
    "\n",
    "1. **Calcul des erreurs absolues**\n",
    "   Pour chaque mod√®le (sauf la r√©gression logistique), on calcule l'erreur absolue entre la valeur r√©elle (`y_test`) et la valeur pr√©dite (`y_pred`) :\n",
    "\n",
    "   $$\n",
    "   \\text{erreur absolue} = |y_{\\text{r√©el}} - y_{\\text{pr√©dit}}|\n",
    "   $$\n",
    "\n",
    "2. **Pr√©vention d'erreur sur `abs()`**\n",
    "   On utilise `del abs` au cas o√π la fonction int√©gr√©e `abs()` aurait √©t√© accidentellement √©cras√©e par une variable appel√©e `abs` dans une cellule pr√©c√©dente.\n",
    "\n",
    "3. **Affichage des histogrammes**\n",
    "   Pour chaque mod√®le, on affiche un histogramme repr√©sentant la distribution des erreurs absolues :\n",
    "\n",
    "   * Un histogramme centr√© vers **0** indique un bon mod√®le.\n",
    "   * Une distribution √©tal√©e ou d√©cal√©e vers la droite indique des erreurs plus fr√©quentes et importantes.\n",
    "\n",
    "Cette visualisation permet de comparer visuellement la **pr√©cision** des mod√®les et d‚Äôidentifier ceux qui font les pr√©dictions les plus proches des vraies valeurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26b945-1f79-4b99-9520-783d3d456602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# S'assurer que abs() est bien une fonction\n",
    "try:\n",
    "    del abs\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Liste pour stocker les erreurs de chaque mod√®le\n",
    "hist_data = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    if \"Logistique\" not in name:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        errors = abs(y_test - y_pred)\n",
    "        hist_data.append((name, errors))\n",
    "\n",
    "# Tracer un histogramme pour chaque mod√®le\n",
    "for name, errors in hist_data:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.hist(errors, bins=30, color='cornflowerblue', alpha=0.7)\n",
    "    plt.title(f\"Histogramme des erreurs absolues ‚Äî {name}\")\n",
    "    plt.xlabel(\"Erreur absolue\")\n",
    "    plt.xlim(0,100)\n",
    "    plt.ylabel(\"Fr√©quence\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7728fe9-b25e-4c20-9d2c-dc9d16396ee5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Analyse des histogrammes des erreurs absolues\n",
    "\n",
    "Les graphiques pr√©sent√©s illustrent la distribution des **erreurs absolues** entre les pr√©dictions des mod√®les et les vraies valeurs de popularit√©. Une erreur absolue faible signifie que le mod√®le a pr√©dit une valeur proche de la r√©alit√©. L‚Äôobjectif est donc d‚Äôavoir une courbe concentr√©e vers z√©ro.\n",
    "\n",
    "---\n",
    "\n",
    "#### R√©gression Lin√©aire\n",
    "\n",
    "* La plupart des erreurs sont **inf√©rieures √† 15**, ce qui indique une bonne approximation globale.\n",
    "* On observe cependant une **queue √©tal√©e** allant jusqu‚Äô√† 50‚Äì60, montrant que certaines pr√©dictions peuvent s‚Äô√©loigner fortement des valeurs r√©elles.\n",
    "* Cela sugg√®re un mod√®le stable mais sensible aux cas extr√™mes.\n",
    "\n",
    "#### R√©gression Ridge\n",
    "\n",
    "* Comportement tr√®s proche de la r√©gression lin√©aire.\n",
    "* L√©g√®re r√©duction des grandes erreurs gr√¢ce √† la r√©gularisation, mais peu de diff√©rence visuelle.\n",
    "* Les erreurs restent mod√©r√©es, sans valeurs aberrantes.\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "* L‚Äôerreur est **tr√®s concentr√©e autour de z√©ro**, ce qui est un bon signe.\n",
    "* Peu d‚Äôerreurs au-del√† de 30, ce qui traduit une **robustesse** du mod√®le face aux variations des donn√©es.\n",
    "* Ce mod√®le offre clairement les **meilleures performances pr√©dictives globales** en termes de pr√©cision.\n",
    "\n",
    "#### R√©gression Polynomiale (degr√© 2)\n",
    "\n",
    "* Distribution similaire aux mod√®les lin√©aires.\n",
    "* Courbe plus irr√©guli√®re, montrant une **sensibilit√© accrue √† certaines zones du domaine**.\n",
    "* Le mod√®le est plus flexible, mais n'apporte pas d‚Äôam√©lioration nette.\n",
    "\n",
    "#### R√©gression Polynomiale (degr√© 3)\n",
    "\n",
    "* La distribution est d‚Äôabord correcte, mais on commence √† voir **quelques erreurs tr√®s √©lev√©es**.\n",
    "* La pr√©sence de valeurs extr√™mes (> 100) est le signe d‚Äôun **surajustement** sur certaines donn√©es.\n",
    "* Le mod√®le devient instable.\n",
    "\n",
    "#### R√©gression Polynomiale (degr√© 4 et plus)\n",
    "\n",
    "* Apparition d‚Äô**erreurs massives** : certaines d√©passent 400, voire 1200.\n",
    "* Cela montre clairement un **effondrement du mod√®le** d√ª √† la complexit√© excessive.\n",
    "* Le mod√®le ne g√©n√©ralise plus du tout : c‚Äôest un exemple typique de **surapprentissage**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82df704-a83f-41ea-a0a1-f7def5d6f949",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üìâ R√©gression logistique ‚Äî classification binaire avec seuil 70\n",
    "\n",
    "Ce bloc de code utilise la **r√©gression logistique** pour transformer le probl√®me de pr√©diction de la popularit√© en une **t√¢che de classification binaire**.\n",
    "\n",
    "#### 1. **Binarisation de la cible `Popularity`**\n",
    "\n",
    "La variable `y` est transform√©e selon un **seuil de 70** :\n",
    "\n",
    "* `1` si la popularit√© est **sup√©rieure ou √©gale √† 70** (chanson tr√®s populaire)\n",
    "* `0` sinon\n",
    "\n",
    "#### 2. **Split des donn√©es**\n",
    "\n",
    "Les ensembles `X_train` et `X_test` sont conserv√©s. La cible binaire (`y_class_70`) est s√©par√©e en `train/test` selon les m√™mes index.\n",
    "\n",
    "#### 3. **Entra√Ænement du mod√®le**\n",
    "\n",
    "Un mod√®le de **r√©gression logistique** est entra√Æn√© sur les donn√©es binaris√©es.\n",
    "\n",
    "#### 4. **√âvaluation**\n",
    "\n",
    "On affiche :\n",
    "\n",
    "* La **matrice de confusion**, qui montre les vrais positifs/n√©gatifs et les erreurs de classification\n",
    "* Le **taux de pr√©cision globale (accuracy)** du mod√®le\n",
    "\n",
    "Cette analyse permet d‚Äô√©valuer si le mod√®le peut efficacement **pr√©dire si une chanson atteindra une forte popularit√©** (‚â•70) √† partir de ses caract√©ristiques audio et √©motionnelles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a49d9-6423-4391-bef5-4f00297dd9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(models)\n",
    "acc_train = [0] * n\n",
    "acc_test = [0] * n\n",
    "acc_cv = [0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0bfda-33fe-417a-9122-e922f4e1a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Cr√©er une version binaire de y avec seuil 70\n",
    "y_class_70 = (y >= 70).astype(int)\n",
    "\n",
    "# 2. Split train/test sur cette version\n",
    "y_train_class70 = y_class_70.loc[X_train.index]\n",
    "y_test_class70 = y_class_70.loc[X_test.index]\n",
    "\n",
    "# 3. Entra√Æner le mod√®le logistique\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train, y_train_class70)\n",
    "y_pred_class70 = log_model.predict(X_test)\n",
    "y_pred_class70_train = log_model.predict(X_train)\n",
    "\n",
    "\n",
    "# 4. Matrice de confusion\n",
    "cm = confusion_matrix(y_test_class70, y_pred_class70)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"< 70\", \"‚â• 70\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Matrice de confusion ‚Äî R√©gression logistique (seuil 70)\")\n",
    "plt.xlabel(\"Pr√©dit\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# 5. Accuracy\n",
    "acc = accuracy_score(y_test_class70, y_pred_class70)\n",
    "acc_train[0] = accuracy_score(y_train_class70, y_pred_class70_train)\n",
    "acc_test[0] = accuracy_score(y_test_class70, y_pred_class70)\n",
    "\n",
    "print(f\"‚úÖ Accuracy (seuil 70) : {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a8bd7-6d58-420a-9a10-1081943bc62c",
   "metadata": {},
   "source": [
    "### Matrices de confusion ‚Äî mod√®les de r√©gression binaris√©s (seuil 70)\n",
    "\n",
    "Ce bloc de code permet d‚Äô√©valuer les mod√®les de r√©gression (autres que la r√©gression logistique) comme des classifieurs binaires, en transformant leur sortie continue en classes √† l‚Äôaide d‚Äôun seuil.\n",
    "\n",
    "#### √âtapes d√©taill√©es :\n",
    "\n",
    "1. **Binarisation de la v√©rit√© terrain (`y_test`)**\n",
    "   La variable cible `Popularity` est transform√©e en variable binaire :\n",
    "\n",
    "   * Valeur 1 si la popularit√© r√©elle est sup√©rieure ou √©gale √† 70\n",
    "   * Valeur 0 sinon\n",
    "\n",
    "2. **Transformation des pr√©dictions**\n",
    "   Chaque mod√®le pr√©dit une valeur num√©rique (`y_pred`).\n",
    "   Ces valeurs sont converties en classes binaires en appliquant un seuil de 70 :\n",
    "\n",
    "   * Classe 1 si `y_pred ‚â• 70`\n",
    "   * Classe 0 sinon\n",
    "\n",
    "3. **√âvaluation via une matrice de confusion**\n",
    "   Pour chaque mod√®le, on compare les pr√©dictions binaris√©es √† la v√©rit√© terrain, sous forme de matrice de confusion.\n",
    "   Celle-ci permet de visualiser :\n",
    "\n",
    "   * Les vrais positifs (bonne d√©tection des chansons populaires)\n",
    "   * Les faux positifs (chansons pr√©dites populaires √† tort)\n",
    "   * Les faux n√©gatifs (chansons populaires non d√©tect√©es)\n",
    "   * Les vrais n√©gatifs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed8e1c-bfc0-40af-94a4-a494edb4117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Binarisation de la v√©rit√© terrain\n",
    "y_test_class70 = (y_test >= 70).astype(int)\n",
    "y_train_class70 = (y_train >= 70).astype(int)\n",
    "i = 1\n",
    "# G√©n√©rer et afficher une matrice de confusion pour chaque mod√®le\n",
    "for name, model in models.items():\n",
    "    if \"Logistique\" not in name:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_continu = model.predict(X_test)\n",
    "        y_pred_continu_train = model.predict(X_train)\n",
    "        print(f\"{name} ‚Äî Pr√©dictions ‚â• 70 : {(y_pred_continu >= 70).sum()}\")\n",
    "        print(f\"Min pr√©diction : {y_pred_continu.min():.2f} | Max : {y_pred_continu.max():.2f} | Moyenne : {y_pred_continu.mean():.2f}\")\n",
    "        y_pred_class70 = (y_pred_continu >= 70).astype(int)\n",
    "        y_pred_class70_train = (y_pred_continu_train >= 70).astype(int)\n",
    "\n",
    "        acc_train[i] = accuracy_score(y_train_class70, y_pred_class70_train)\n",
    "        acc_test[i] = accuracy_score(y_test_class70, y_pred_class70)\n",
    "\n",
    "        # Matrice de confusion\n",
    "        cm = confusion_matrix(y_test_class70, y_pred_class70)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"< 70\", \"‚â• 70\"])\n",
    "        disp.plot(cmap=\"Blues\")\n",
    "        plt.title(f\"Matrice de confusion ‚Äî {name} (seuil 70)\")\n",
    "        plt.xlabel(\"Pr√©dit\")\n",
    "        plt.ylabel(\"R√©el\")\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n",
    "        i+=1\n",
    "\n",
    "print(\"Taille y_test_class70 :\", len(y_test_class70))\n",
    "print(\"Taille y_pred_class70 :\", len(y_pred_class70))\n",
    "print(\"Somme matrice :\", cm.sum())\n",
    "print(\"Taille test :\", len(y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0bbbe-679a-45d3-b2dd-5e6cdc8a211a",
   "metadata": {},
   "source": [
    "\n",
    "### Analyse des matrices de confusion (seuil = 30)\n",
    "\n",
    "Nous √©valuons ici plusieurs mod√®les de r√©gression en les transformant en classifieurs binaires, en consid√©rant qu'une chanson est *populaire* si sa popularit√© pr√©dite est sup√©rieure ou √©gale √† 30. Les r√©sultats sont exprim√©s sous forme de **matrices de confusion**, accompagn√©es de statistiques descriptives (minimum, maximum et moyenne des pr√©dictions).\n",
    "\n",
    "#### R√©gression Lin√©aire\n",
    "\n",
    "* Le mod√®le a tendance √† pr√©dire beaucoup de valeurs sup√©rieures √† 30 (1248 cas), mais il produit un grand nombre de **faux positifs**.\n",
    "* Il d√©tecte tout de m√™me correctement 55 chansons r√©ellement populaires sur 68, ce qui montre un bon **rappel**, mais au prix d‚Äôune faible **pr√©cision**.\n",
    "\n",
    "#### R√©gression Ridge\n",
    "\n",
    "* Ce mod√®le affiche un comportement extr√™me : il pr√©dit quasiment toutes les valeurs au-dessus du seuil (1743 sur 2000), ce qui lui permet de capturer presque toutes les vraies chansons populaires (63 sur 68).\n",
    "* En contrepartie, le nombre de **faux positifs** est tr√®s √©lev√©, rendant les pr√©dictions peu fiables pour discriminer correctement.\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "* Ce mod√®le offre un bon **√©quilibre** entre rappel et pr√©cision.\n",
    "* Il parvient √† d√©tecter 54 vraies chansons populaires tout en limitant mieux les faux positifs que les mod√®les lin√©aires.\n",
    "* Sa capacit√© √† mod√©liser des relations non lin√©aires semble lui permettre de mieux s√©parer les classes.\n",
    "\n",
    "#### R√©gression Polynomiale (degr√© 2)\n",
    "\n",
    "* Le comportement est similaire √† celui du Random Forest, mais l√©g√®rement moins √©quilibr√©.\n",
    "* Elle capte 51 chansons populaires avec un peu plus de faux positifs, ce qui refl√®te un compromis correct entre complexit√© et performance.\n",
    "\n",
    "#### R√©gression Polynomiale (degr√© 3)\n",
    "\n",
    "* Ce mod√®le montre des pr√©dictions tr√®s instables, avec des valeurs extr√™mes allant de ‚Äì400 √† plus de 200.\n",
    "* Malgr√© cela, il d√©tecte 48 chansons populaires, mais reste moins performant que le degr√© 2 ou le Random Forest.\n",
    "* Cela sugg√®re un d√©but de **surapprentissage**, typique des mod√®les polynomiaux trop complexes.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Au seuil de 30, **le mod√®le Random Forest est celui qui montre le meilleur compromis** entre d√©tection correcte des chansons populaires et limitation des erreurs. Les mod√®les lin√©aires, bien qu‚Äôefficaces en rappel, souffrent d‚Äôun trop grand nombre de faux positifs, tandis que les mod√®les polynomiaux deviennent instables au-del√† du degr√© 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ee164-982a-4d9b-86f8-90dab6089fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_test,acc_train)\n",
    "model_names = [\"R√©gression Logistique\",\"R√©gression Lin√©aire\",\"R√©gression Ridge\",\"Random Forest\",\"R√©gression KNN\",\"R√©gression Polynomiale (deg=2)\",\"R√©gression Polynomiale (deg=3)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4802746-2694-43fd-afb1-a4a2db6e8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position des barres\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "# Cr√©ation du graphique\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Barres\n",
    "ax.bar(x - width/2, acc_train, width, label='Train', color='lightgreen')\n",
    "ax.bar(x + width/2, acc_test, width, label='Test', color='skyblue')\n",
    "\n",
    "# Mise en forme\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Comparaison Accuracy - Train vs Cross-Val vs Test')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5b813-ffd5-415a-b321-a59d49b98c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
